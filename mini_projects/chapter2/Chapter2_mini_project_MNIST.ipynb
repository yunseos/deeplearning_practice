{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8421af-5269-4f1e-b226-064755fd0d6a",
   "metadata": {},
   "source": [
    "### 1. 필요 모듈 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6fd5379-2de6-471b-8c34-7e8c4d73ab08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920b175c-05e1-421b-873c-070fc418c91f",
   "metadata": {},
   "source": [
    "### 2. MNIST 훈련, 테스트 데이터셋 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23608297-f5cc-4486-bf6b-01877cdb99f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset.MNIST 를 사용해서 MNIST 데이터셋을 다운받을 수 있다. train 은 훈련데이터 여부, download는 기존에 없다면 root 경로에 다운로드)\n",
    "train_dataset = datasets.MNIST(root='/home/yunseo/notebooks/deeplearning_practice/data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='/home/yunseo/notebooks/deeplearning_practice/data', train=False, transform=transforms.ToTensor()) #코드가 비슷해보여도 train이 False여서 이건 test 데이터가 다운로드 됨!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7ca4f-7240-4351-b913-ac539d35339d",
   "metadata": {},
   "source": [
    "### 3. 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0fe50cf-96d3-48d2-a42b-d437ca2b7ca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0\n",
      "Size of Image: (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(70)\n",
    "\n",
    "idx = torch.randint(0, len(train_dataset), (1,)).item()\n",
    "random_img = train_dataset[idx][0].squeeze().numpy()\n",
    "target_num = train_dataset[idx][1]\n",
    "\n",
    "print(\"Target: {}\".format(target_num))\n",
    "print(\"Size of Image: {}\".format(random_img.shape))\n",
    "\n",
    "plt.imshow(random_img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792668ed-ec9f-4779-b34b-e33bf85e6277",
   "metadata": {},
   "source": [
    "### 4. 데이터 로더 정의, 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a909b141-b56d-4de7-923c-ed1e50a51f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 64 #미니배치의 크기. 한번에 얼마나 많은 양의 이미지를 학습시킬건지\n",
    "DEVICE = 'cpu'\n",
    "STEP =10\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100f9a76-3048-4da1-bb5f-8fe345bafb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for (data, target) in train_loader:\n",
    "    print(data.size(), target.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f5010-41ed-489b-915a-55dbebc94d8b",
   "metadata": {},
   "source": [
    "### 5. 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1cb288c-a64e-4a5a-98c7-307f3d9c9463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.flatten = lambda x: x.view(x.size(0), -1)\n",
    "        self.linear_ih = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_hh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_ho = nn.Linear(hidden_size, output_size)\n",
    "        self.activation_layer = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_ih(x)\n",
    "        x = self.activation_layer(x)\n",
    "        x = self.linear_hh(x)\n",
    "        x = self.activation_layer(x)\n",
    "        x = self.linear_ho(x)\n",
    "        y = self.activation_layer(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a74ab76a-bcb1-48b2-bbe6-48b5328a8bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(input_size=28*28, hidden_size=100, output_size=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46c3ba-0681-4929-aaca-cda3595eda67",
   "metadata": {},
   "source": [
    "### 6. 손실함수 및 옵티마이저 정의, 파라미터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a818f2f7-dbd1-4d0c-bb1e-9f730751853d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 파라미터 총 개수: 89610\n"
     ]
    }
   ],
   "source": [
    "#옵티마이저에는 모델에 내장되어있는 모든 매개변수를 인자로 전달해줘야한다.\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "num_params = 0 #모델에 사용되는 파라미터 확인위한 초기화\n",
    "for params in model.parameters():\n",
    "    num_params += params.view(-1).size(0)\n",
    "print('모델 파라미터 총 개수: {}'.format(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4c65c-2b52-4a6c-8341-f160df0d8b4a",
   "metadata": {},
   "source": [
    "### 7. 학습 및 테스트에 필요한 함수 정의\n",
    "- train 함수는 한 스텝 동안 발생하는 학습 과정\n",
    "- test 함수는 테스트 데이터로 모델의 성능을 결정하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4002d5e-fc50-4099-9fcd-60d50340fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader, loss_func, optimizer, step, device, print_step=200):\n",
    "    \"\"\"train function: 1 스텝 동안 발생하는 학습 과정\"\"\"\n",
    "    model.train() #모델한테 훈련단계라고 선언\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device) #입력과 타겟을 동일한 디바이스에 올려줘야 됨\n",
    "\n",
    "        #forward\n",
    "        model.zero_grad() #경사 초기화\n",
    "        output = model(data)\n",
    "        loss = loss_func(output, target)\n",
    "\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % print_step ==0:\n",
    "            print('Train Step: {} ({:05.2f}%) \\tLoss: {:.4f}' .format(step, 100.*(batch_idx*train_loader.batch_size)/len(train_loader.dataset), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, test_loader, loss_func, device):\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "    correct=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_func(output, target, reduction=\"sum\").item()\n",
    "            pred = output.softmax(1).argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() #item은 텐서를 int로 변환\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_acc = correct / len(test_loader.dataset)\n",
    "        print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:05.2f}%'.format(test_loss, correct, len(test_loader.dataset), 100. *test_acc))\n",
    "        return test_loss, test_acc\n",
    "\n",
    "def main(model, train_loader, test_loader, loss_func, optimizer, n_step, device, save_path, print_step):\n",
    "    test_accs = []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for step in range(1, n_step+1):\n",
    "        train(model, train_loader, loss_func, optimizer, step=step, device=device, print_step=print_step)\n",
    "        test_loss, test_acc =test(model, test_loader, loss_func=F.cross_entropy, device=device)\n",
    "\n",
    "        test_accs.append(test_acc)\n",
    "        if len(test_accs) >= 2:\n",
    "            if test_acc >=best_acc:\n",
    "                best_acc = test_acc\n",
    "                best_state_dict = model.state_dict()\n",
    "                print(\"discard previous state, best model state saved!\")\n",
    "        print(\"\")\n",
    "\n",
    "    torch.save(best_state_dict, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd73aed-2ce7-4cf7-96c1-1df7bf7c4c4c",
   "metadata": {},
   "source": [
    "### 8. 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "296f01e2-2ec9-43c9-aaef-48415bed8779",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 1 (00.00%) \tLoss: 2.2989\n",
      "Train Step: 1 (21.33%) \tLoss: 1.2488\n",
      "Train Step: 1 (42.67%) \tLoss: 1.2408\n",
      "Train Step: 1 (64.00%) \tLoss: 1.3360\n",
      "Train Step: 1 (85.33%) \tLoss: 1.3268\n",
      "Test set: Average loss: 1.2444, Accuracy: 5815/10000 (58.15%\n",
      "\n",
      "Train Step: 2 (00.00%) \tLoss: 1.4519\n",
      "Train Step: 2 (21.33%) \tLoss: 1.2959\n",
      "Train Step: 2 (42.67%) \tLoss: 1.2112\n",
      "Train Step: 2 (64.00%) \tLoss: 1.3419\n",
      "Train Step: 2 (85.33%) \tLoss: 1.1580\n",
      "Test set: Average loss: 1.1896, Accuracy: 5961/10000 (59.61%\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 3 (00.00%) \tLoss: 1.0241\n",
      "Train Step: 3 (21.33%) \tLoss: 0.9561\n",
      "Train Step: 3 (42.67%) \tLoss: 1.2139\n",
      "Train Step: 3 (64.00%) \tLoss: 1.2130\n",
      "Train Step: 3 (85.33%) \tLoss: 1.1178\n",
      "Test set: Average loss: 1.1792, Accuracy: 6000/10000 (60.00%\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 4 (00.00%) \tLoss: 1.5041\n",
      "Train Step: 4 (21.33%) \tLoss: 1.5013\n",
      "Train Step: 4 (42.67%) \tLoss: 1.2607\n",
      "Train Step: 4 (64.00%) \tLoss: 1.1915\n",
      "Train Step: 4 (85.33%) \tLoss: 1.3418\n",
      "Test set: Average loss: 1.1745, Accuracy: 6007/10000 (60.07%\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 5 (00.00%) \tLoss: 1.0878\n",
      "Train Step: 5 (21.33%) \tLoss: 1.0236\n",
      "Train Step: 5 (42.67%) \tLoss: 0.9998\n",
      "Train Step: 5 (64.00%) \tLoss: 1.1528\n",
      "Train Step: 5 (85.33%) \tLoss: 1.3729\n",
      "Test set: Average loss: 1.1756, Accuracy: 6013/10000 (60.13%\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 6 (00.00%) \tLoss: 1.0392\n",
      "Train Step: 6 (21.33%) \tLoss: 1.1549\n",
      "Train Step: 6 (42.67%) \tLoss: 1.1467\n",
      "Train Step: 6 (64.00%) \tLoss: 1.0846\n",
      "Train Step: 6 (85.33%) \tLoss: 1.1205\n",
      "Test set: Average loss: 1.1829, Accuracy: 6032/10000 (60.32%\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 7 (00.00%) \tLoss: 1.3866\n",
      "Train Step: 7 (21.33%) \tLoss: 1.3011\n",
      "Train Step: 7 (42.67%) \tLoss: 1.1895\n",
      "Train Step: 7 (64.00%) \tLoss: 1.4329\n",
      "Train Step: 7 (85.33%) \tLoss: 1.1115\n",
      "Test set: Average loss: 1.1725, Accuracy: 6023/10000 (60.23%\n",
      "\n",
      "Train Step: 8 (00.00%) \tLoss: 1.1353\n",
      "Train Step: 8 (21.33%) \tLoss: 1.4066\n",
      "Train Step: 8 (42.67%) \tLoss: 1.1211\n",
      "Train Step: 8 (64.00%) \tLoss: 1.2389\n",
      "Train Step: 8 (85.33%) \tLoss: 1.0135\n",
      "Test set: Average loss: 1.1774, Accuracy: 6040/10000 (60.40%\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 9 (00.00%) \tLoss: 1.2677\n",
      "Train Step: 9 (21.33%) \tLoss: 1.0207\n",
      "Train Step: 9 (42.67%) \tLoss: 1.0813\n",
      "Train Step: 9 (64.00%) \tLoss: 1.1914\n",
      "Train Step: 9 (85.33%) \tLoss: 1.3326\n",
      "Test set: Average loss: 1.1691, Accuracy: 6057/10000 (60.57%\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 10 (00.00%) \tLoss: 1.1506\n",
      "Train Step: 10 (21.33%) \tLoss: 1.0848\n",
      "Train Step: 10 (42.67%) \tLoss: 1.4279\n",
      "Train Step: 10 (64.00%) \tLoss: 1.2237\n",
      "Train Step: 10 (85.33%) \tLoss: 1.0413\n",
      "Test set: Average loss: 1.1617, Accuracy: 6042/10000 (60.42%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main (model=model,\n",
    "      train_loader=train_loader,\n",
    "      test_loader=test_loader,\n",
    "      loss_func=loss_function,\n",
    "      optimizer=optimizer,\n",
    "      n_step=STEP,\n",
    "      device=DEVICE,\n",
    "      save_path=\"mnist_model.pt\",\n",
    "      print_step=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f79654-0c64-4a21-be70-91020ea7cc4c",
   "metadata": {},
   "source": [
    "### 9. 내가 그린 손글씨로 테스트\n",
    "\n",
    "- local에서 그린 숫자를 전처리하여 사용 (GUI 에러)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "181949a0-e33f-4513-83ac-63c8af33b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_number_image(image_path, save_dir=\"./processed_figs/\"):\n",
    "#     \"\"\"저장된 숫자 이미지를 전처리 (28x28 크기, 흑백 변환, 색 반전) 후 자동 저장\"\"\"\n",
    "    \n",
    "#     if not os.path.exists(image_path):\n",
    "#         raise FileNotFoundError(f\"File not found error: {image_path}\")\n",
    "\n",
    "#     filename = os.path.basename(image_path)  # 이미지명 추출\n",
    "#     number_part = os.path.splitext(filename)[0]  # 확장자 빼고 분리\n",
    "\n",
    "#     # 저장 디렉토리 생성 (없으면 자동 생성)\n",
    "#     if not os.path.exists(save_dir):\n",
    "#         os.makedirs(save_dir)\n",
    "\n",
    "#     # 저장할 파일 경로 설정\n",
    "#     save_path = os.path.join(save_dir, f\"processed_{number_part}.png\")\n",
    "\n",
    "#     # 이미지 불러오기\n",
    "#     img = Image.open(image_path).convert(\"L\")  # 흑백 변환\n",
    "#     img = ImageOps.invert(img)  # 색 반전 (검은 배경 → 흰색 배경)\n",
    "#     img = img.resize((28, 28))  # 28x28 크기로 변환\n",
    "\n",
    "#     # 변환된 이미지 저장\n",
    "#     img.save(save_path)\n",
    "#     print(f\"전처리된 이미지가 저장되었습니다: {save_path}\")\n",
    "\n",
    "#     # 이미지 출력\n",
    "#     plt.imshow(img, cmap=\"gray\")\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "\n",
    "#     return img\n",
    "\n",
    "\n",
    "# preprocessed_img = preprocess_number_image(\"./figs/9.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d688d55-27eb-43e2-899b-f91bb78a1150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted number is 3.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "img_path = \"./processed_figs/processed_9.png\"\n",
    "# img_path = \"./figs/number.png\"\n",
    "img = Image.open(img_path)\n",
    "test_input = torch.Tensor(np.array(img)).unsqueeze(0).to(DEVICE)\n",
    "pred = model(test_input)\n",
    "\n",
    "print(\"Predicted number is {}.\".format(pred.softmax(1).argmax().item()))\n",
    "\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e2967-01c2-48bf-b0a6-d00929e1e2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(torchenv)",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
